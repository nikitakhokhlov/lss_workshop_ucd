[
  {
    "objectID": "tutorial/lss_tutorial.html",
    "href": "tutorial/lss_tutorial.html",
    "title": "Introduction to Latent Semantic Scaling",
    "section": "",
    "text": "Latent Semantic Scaling (LSS) Watanabe (2021) is a semi-supervised word-embedding scaling technique that allows to locate documents on pre-defined dimensions of your interest. The user provides seed words to define the scale (e.g., sentiment, hostility). Then, the algorithm estimates polarity of words in the corpus and locates documents on a unidimensional scale.\nThis tutorial is drawing on the tutorials by Kohei Watanabe: Introduction to LSX and Latent Semantic Scaling."
  },
  {
    "objectID": "tutorial/lss_tutorial.html#introduction",
    "href": "tutorial/lss_tutorial.html#introduction",
    "title": "Introduction to Latent Semantic Scaling",
    "section": "",
    "text": "Latent Semantic Scaling (LSS) Watanabe (2021) is a semi-supervised word-embedding scaling technique that allows to locate documents on pre-defined dimensions of your interest. The user provides seed words to define the scale (e.g., sentiment, hostility). Then, the algorithm estimates polarity of words in the corpus and locates documents on a unidimensional scale.\nThis tutorial is drawing on the tutorials by Kohei Watanabe: Introduction to LSX and Latent Semantic Scaling."
  },
  {
    "objectID": "tutorial/lss_tutorial.html#installation",
    "href": "tutorial/lss_tutorial.html#installation",
    "title": "Introduction to Latent Semantic Scaling",
    "section": "Installation",
    "text": "Installation\nFrom CRAN:\n\n#install.packages(\"LSX\")\n\nFrom Github:\n\n#devtools::install_github(\"koheiw/LSX\")\n\n\n#install.packages(c('quanteda', 'quanteda.corpora', \n#           'quanteda.textstats', 'quanteda.textplots', \n#           'dplyr', 'ggplot2', 'geomtextpath', 'wordcloud', \n#           'plm', 'modelsummary'))"
  },
  {
    "objectID": "tutorial/lss_tutorial.html#loading-required-packages",
    "href": "tutorial/lss_tutorial.html#loading-required-packages",
    "title": "Introduction to Latent Semantic Scaling",
    "section": "Loading required packages",
    "text": "Loading required packages\n\npacks &lt;- c('LSX', 'quanteda', 'quanteda.corpora', \n           'quanteda.textstats', 'quanteda.textplots', \n           'dplyr', 'ggplot2', 'geomtextpath', 'wordlcloud', \n           'plm', 'modelsummary')\n\n# Install missing packages\n#installed &lt;- packs %in% rownames(installed.packages())\n#if (any(!installed)) install.packages(packs[!installed])\n\nlapply(packs, require, character.only = TRUE)\n\nLoading required package: LSX\n\n\nLoading required package: quanteda\n\n\nPackage version: 3.3.1\nUnicode version: 14.0\nICU version: 70.1\n\n\nParallel computing: 4 of 4 threads used.\n\n\nSee https://quanteda.io for tutorials and examples.\n\n\nLoading required package: quanteda.corpora\n\n\nLoading required package: quanteda.textstats\n\n\nLoading required package: quanteda.textplots\n\n\nLoading required package: dplyr\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nLoading required package: ggplot2\n\n\nLoading required package: geomtextpath\n\n\nLoading required package: wordlcloud\n\n\nWarning in library(package, lib.loc = lib.loc, character.only = TRUE,\nlogical.return = TRUE, : there is no package called 'wordlcloud'\n\n\nLoading required package: plm\n\n\n\nAttaching package: 'plm'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, lag, lead\n\n\nThe following object is masked from 'package:quanteda':\n\n    index\n\n\nLoading required package: modelsummary\n\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\n[[3]]\n[1] TRUE\n\n[[4]]\n[1] TRUE\n\n[[5]]\n[1] TRUE\n\n[[6]]\n[1] TRUE\n\n[[7]]\n[1] TRUE\n\n[[8]]\n[1] TRUE\n\n[[9]]\n[1] FALSE\n\n[[10]]\n[1] TRUE\n\n[[11]]\n[1] TRUE"
  },
  {
    "objectID": "tutorial/lss_tutorial.html#corpus-1-russian-propaganda-on-ukraine",
    "href": "tutorial/lss_tutorial.html#corpus-1-russian-propaganda-on-ukraine",
    "title": "Introduction to Latent Semantic Scaling",
    "section": "Corpus 1: Russian propaganda on Ukraine",
    "text": "Corpus 1: Russian propaganda on Ukraine\n\nPreparing the text corpus\nWe will first work with the corpus of news from the Russian propaganda outlet Sputnik contaning the word Ukraine in 2022, provided by Kohei Watanabe: download here (save it in the same working directory as this .qmd file).\n\ncorp &lt;- readRDS(\"data/data_corpus_sputnik2022.rds\")\n\nCheck the number of documents in the text corpus:\n\nndoc(corp)\n\n[1] 8063\n\n\nLet’s check the first text:\n\nas.character(corp)[1]\n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           s1092644731 \n\"On Monday, US President Joe Biden reiterated the United States' commitment to diplomacy as tensions between Russia, Ukraine, and NATO worsened. \\n\\nBiden told reporters\\n\\n, \\\"We continue to urge diplomacy as the best way forward, but with Russia continuing its buildup of its forces around Ukraine, we are ready no matter what happens\\\".\\n\\nBiden spoke with Ukrainian President Volodymyr Zelensky last week, and the president continues what he referred to as \\\"non-stop diplomacy\\\".\\n\\n\\\"I had a productive talk last week with President Zelensky, and we continue to engage in non-stop diplomacy\\\".\\n\\nAccording to\\n\\n CNN\\n\\n, however, the purported \\\"productive\\\" talks were not well-received by Biden's Ukrainian counterpart. A senior Ukrainian official claimed that there is a disconnect over the difference between the two sides' risk in the event of a Russian invasion. \\n\\nWhite House National Security Council spokesperson Emily Horne dismissed reports of a disconnect between Kiev and Washington as false.\\n\\nThe White House confirmed they have sanction packages ready, should the situation call for it, that target Russian elites and their families. \\n\\nAccording to the White House\\n\\n, the targets \\\"are in or near the inner circle of the Kremlin, play a role in government decision making or at a minimum complicit in the Kremlin's destabilising behaviour\\\".\\n\\nBiden Backs Move to Designate Qatar as 'Major Non-NATO Ally'\\n\\nOn Monday, Biden said he would notify Congress of his intention to designate Qatar as a key non-NATO ally. He is said to have made the decision while meeting with Qatar's emir, Sheikh Tamim bin Hamad al-Thani, in the Oval Office. \\n\\nThe two reportedly discussed regional security and a push for equal rights for Palestinians. \\n\\n\\n\\n\\n\\n\\n\\n.⁦\\n\\n@POTUS\\n\\n⁩, in meeting with emir of Qatar, says he wants diplomacy with Moscow but as Russia continues to build up forces along the Ukraine border, the U.S. is ready for anything. \\n\\npic.twitter.com/z279wlev7i\\n\\n— Jeff Mason (@jeffmason1) \\n\\nJanuary 31, 2022\\n\\n\\n\\n\\nEurope and several NATO allies are gripped by an energy crisis. Russia supplies over 30% of Europe's natural gas and is the continent's most crucial source of energy. With the threat of sanctions and a potential conflict over the purported buildup of Russian troops along the Ukrainian border, Europe and NATO have looked to find alternative energy exporters to meet their demand. Qatar has been one of the nations \\n\\nreportedly chosen\\n\\n to help ease Europe's energy crunch.\\n\\nPentagon: Troops on 'Heightened Alert' Over Ukraine Crisis Have Not Been Given Deployment Orders\\n\\nLast week, the United States military put 8,500 troops stationed in the US on \\\"heightened alert\\\" for deployment to Eastern Europe and the Baltic States, according to Pentagon press secretary John Kirby. \\n\\n\\\"They have not been given deployment orders\\\", Kirby said. \\\"They've just been told to be ready on a shorter period of time in case the alliance activates that\\\".\\n\\nAccording to Kirby, the troops could be deployed in five days if called upon. US troops already stationed in Europe could also be called up. The majority of troops are ground forces and would be part of a NATO response force in the event, according to Washington and its allies, that Russia engaged Ukraine in a military conflict.\\n\\nKirby told reporters the Pentagon's decision came down to a belief that \\\"it is very clear the Russians have no intensions of de-escalating\\\".\\n\\nA deployment of troops would reportedly not be a US military decision, but rather a NATO decision, according to Kirby. \\n\\nUS Open Talks Proposal on Ukraine 'Classic Example of Megaphone Diplomacy'\\n\\nThe US has approached the United Nations in an attempt to gain international support over their allegations that a Russian invasion of Ukraine is an imminent threat. The Kremlin has regularly denied the continuing accusations and has cited NATO's increased military presence on its borders over the past decade as a clear double standard. \\n\\nThe US ambassador to the UN, Linda Thomas-Greenfield, implored the UN Security Council to look at Russia's actions and not to take Kremlin statements at face value. She added, however, that diplomacy is the preferred course of action. \\n\\nWashington has asked Moscow to take part in a public meeting to discuss security concerns in the region.\\n\\n\\\"The United States has been clear. If this is truly about Russia's security concerns in Europe, we're offering them an opportunity to address these concerns at the negotiating table\\\", Thomas-Greenfield said.\\n\\nRussia's ambassador to the UN, Vassily Nebenzia, called the US request for an open meeting a \\n\\n\\\"classic example of megaphone diplomacy\\\".\\n\\nHe observed that the United States and its allies have failed to provide evidence that an attack is imminent and believes Washington is actually pushing for a worst-case scenario, armed conflict, to occur, as a means of justifying an increased global military footprint. \\n\\n\\\"Talks about the coming war are provocative by themselves.  seems to be calling for this, wanting and waiting for  to happen, as if you want to make your speculations come true\\\", Nebenzia pointed out.\" \n\n\nReshape the corpus to sentences, tokenize and pre-process it: remove punctuation, non-textual symbols, numbers, URLs, and English stop words (grammatical words that appear often in the text):\n\ncorp_sent &lt;- corpus_reshape(corp, to =  \"sentences\") # reshape to sentences\n\ntoks_sent &lt;- corp_sent %&gt;%\n    tokens(remove_punct = TRUE, remove_symbols = TRUE,  \n           remove_numbers = TRUE, remove_url = TRUE) %&gt;% \n    tokens_remove(stopwords(\"en\"))\n\nCreate a document-feature matrix:\n\ndfmt &lt;- dfm(toks_sent) |&gt; \n    dfm_remove(pattern = \"\") |&gt; # remove space from the DFM\n    dfm_trim(min_termfreq = 5) # trim the DFM from the features that rarely appear (e.g., less than 5 times across all documents)\n\n\n\nExample 1: Sentiment\nFirst, let’s evaluate the general sentiment of Russian news articles on Ukraine over 2022.\ndata_dictionary_sentiment is the built-in dictionary of sentiment seed words.\nas.seedwords() converts the dictionary object to a named numeric vector, in which numbers indicate seed words’ polarity (positive: 1 or negative: -1).\n\nseed_sentiment &lt;- LSX::as.seedwords(data_dictionary_sentiment)\nprint(seed_sentiment)\n\n       good        nice   excellent    positive   fortunate     correct \n          1           1           1           1           1           1 \n   superior         bad       nasty        poor    negative unfortunate \n          1          -1          -1          -1          -1          -1 \n      wrong    inferior \n         -1          -1 \n\n\ntextmodel_lss() computes the polarity scores of all the words in the corpus based on their semantic similarity to the seed words.\n\nlss_sentiment &lt;- textmodel_lss(dfmt, seeds = seed_sentiment)\n\nLet’s take a look at the top-20 words associated with the “positive” pole.\n\nas.data.frame(head(coef(lss_sentiment), 20))\n\n               head(coef(lss_sentiment), 20)\nexcellent                          0.2145698\ngood                               0.1854292\npositive                           0.1816735\nmend                               0.1768906\ndiplomatic                         0.1740417\ncorrect                            0.1717065\nrusso-american                     0.1648571\nrelations                          0.1548362\ncordial                            0.1527409\nmutual                             0.1512451\nunderstanding                      0.1492428\nsoured                             0.1481486\nreaffirmed                         0.1479711\nfriendly                           0.1475290\nsuperior                           0.1466709\n70th                               0.1464543\nnormalise                          0.1445001\nnice                               0.1444575\nmaintain                           0.1436995\nnormalising                        0.1412014\n\n\nNow see top-20 words associated with the “negative” pole.\n\nas.data.frame(tail(coef(lss_sentiment), 20))\n\n                   tail(coef(lss_sentiment), 20)\nmitigate                              -0.1124128\ncontributes                           -0.1130591\nba                                    -0.1135220\nanti-trump                            -0.1137989\npandemic                              -0.1138519\nhiroshima                             -0.1147711\nclass                                 -0.1154528\ndisproportionately                    -0.1174913\njosh                                  -0.1176860\nnovel                                 -0.1236345\nsubvariant                            -0.1272572\nthreatens                             -0.1286198\nnegative                              -0.1289053\nwatson                                -0.1502266\ndeshaun                               -0.1513966\nnfl                                   -0.1517632\npoor                                  -0.1597352\nwrong                                 -0.1610638\nspells                                -0.1662341\nunfortunate                           -0.1696293\n\n\nYou can visualize the polarity of words using textplot_terms().\nWhen highlighted = NULL (default option), `textplot_terms` randomly samples 50 words and highlights them.\n\ntextplot_terms(lss_sentiment)\n\nWarning: ggrepel: 1 unlabeled data points (too many overlaps). Consider\nincreasing max.overlaps\n\n\n\n\n\n\n\n\n\nYou can also manually specify which words (or emoji) to highlight. You can pass glob patterns to highlighted if you want.\n\ntextplot_terms(lss_sentiment, highlighted = c('biden', 'putin', names(seed_sentiment)))\n\n\n\n\n\n\n\n\nBefore predicting polarity of documents, we should reconstruct original full texts of articles from their sentences using dfm_group().\n\ndfmt_doc &lt;- dfm_group(dfmt)\ndat &lt;- docvars(dfmt_doc)\ndat$docname &lt;- docnames(dfmt_doc)\nprint(nrow(dat))\n\n[1] 8063\n\n\nOverall, we have 8,063 documents - news articles in English containing the word Ukraine from the Russian propaganda outlet Sputnik, over 2022.\nNote that we can also run the initial textmodel_lss() estimation with non-default parameters.\n\nWhen both include_data and group_data are TRUE (both FALSE by default), it internally applies dfm_group() to x to group the sentences into the original documents, effectively reversing the segmentation by corpus_reshape(), and save a grouped DFM in the LSS object as lss_sentiment$data. In such a case, we do not need to “reconstruct” original full texts from sentences, as we did above.\nIf cache = TRUE (FALSE by default), an intermediate object is saved in a folder lss_cache in the working directory.\nk = 300 (by default) is the number of singular values requested to the singular value decomposition (SVD) engine.\n\n(Watanabe 2021, 88): “The literature suggests that the optimal value of k in SVD is around 200–300 for synonym identification”.\nSee also the “Hyperparameter Optimization” section in (Watanabe 2021, 96–97)\n\n\n\nlss_sentiment2 &lt;- textmodel_lss(dfmt, seeds = seed_sentiment, \n                               k = 300, \n                               include_data = TRUE, \n                               group_data = TRUE,\n                               cache = TRUE)\n\nWriting cache file: lss_cache/svds_880d3cd3883e22e4.RDS\n\n\nNow, let’s predict polarity of documents.\n\ndat$lss_sentiment &lt;- predict(lss_sentiment, newdata = dfmt_doc)\n\nTo visualize the polarity of documents, you need first to smooth their scores using smooth_lss()1.\n\nsmo &lt;- smooth_lss(dat, lss_var = \"lss_sentiment\", date_var = \"date\")\n\nNow, we visualize the sentiment of news articles over time, with 95% confidence intervals.\n\nggplot(smo, aes(x = date, y = fit)) + \n    geom_line() +\n    geom_ribbon(aes(ymin = fit - se.fit * 1.96, ymax = fit + se.fit * 1.96), alpha = 0.1) +\n    geom_vline(xintercept = as.Date(\"2022-02-24\"), linetype = \"dotted\") +\n    scale_x_date(date_breaks = \"months\", date_labels = \"%b\") +\n    labs(title = \"Sentiment about articles on Ukraine\", x = \"Date\", y = \"Sentiment\") +\n    theme_bw()\n\n\n\n\n\n\n\n\n\n\nExample 2: Hostility towards Ukraine\nFollowing Trubowitz and Watanabe (2021), we can estimate hostility towards Ukraine expressed in the Sputnik articles in 2022. You can download the hostility dictionary file from the Github repository.\n\ndict &lt;- dictionary(file = \"data/dictionary_hostility.yml\") \nprint(dict$hostility)\n\nDictionary object with 2 key entries.\n- [hostile]:\n  - adversary, adversaries, enemy, enemies, foe, foes, hostile\n- [friendly]:\n  - aid, aids, friends, friend, ally, allies, peaceful\n\n\nWe convert the dictionary into a named numeric vector, in which numbers indicate seed words’ polarity.\n\nhostility &lt;- as.seedwords(dict$hostility)\n\nTo target hostility towards Ukraine, you should assign polarity scores only to words that occur around “ukrain*” as its modifiers. You can collect such words using char_context() and pass them to textmodel_lss() through terms.\ngroup_data is set to FALSE because we want to analyze each sentence in this example.\n\nterm &lt;- toks_sent |&gt; char_context(pattern = \"ukrain*\", p = 0.01)\nlss_hostility &lt;- textmodel_lss(dfmt, seeds = hostility, terms = term, \n                               include_data = TRUE)\n\n\nas.data.frame(head(coef(lss_hostility), 10))\n\n            head(coef(lss_hostility), 10)\ninformation                     0.1679121\nmalware                         0.1495801\nfakes                           0.1490043\nhackers                         0.1450184\noperations                      0.1274739\noperatives                      0.1206912\ntransferred                     0.1161760\nrahdit                          0.1145856\nnationalism                     0.1089410\nfalse                           0.1082845\n\n\n\nas.data.frame(tail(coef(lss_hostility), 10))\n\n             tail(coef(lss_hostility), 10)\nfunneled                       -0.09653453\nfacilitating                   -0.10158606\nresolving                      -0.11093048\nlethal                         -0.11618711\nhumanitarian                   -0.12255891\nbillions                       -0.12330203\npackage                        -0.12614307\nspartz                         -0.14455401\nrecipient                      -0.15838429\naid                            -0.17494417\n\n\n\ntextplot_terms(lss_hostility)\n\n\n\n\n\n\n\n\nWe can compute the polarity scores of documents using predict() with min_n to avoid short sentences to receive extremely large negative or positive scores (i.e. outliers).\npredict() tends to return extreme scores for short sentences because it computes the polarity of documents based on the polarity of words weighted by their frequency. To prevent a small number of words from determining the document scores, we set min_n = 15, which is roughly the first quantile of the sentence lengths.\n\ndat2 &lt;- docvars(lss_hostility$data)\nquantile(ntoken(lss_hostility$data))\n\n  0%  25%  50%  75% 100% \n   0    9   13   19  167 \n\ndat2$lss_hostility &lt;- predict(lss_hostility, min_n = 15)\n\nYou can use smooth_lss() to visualize the scores, but engine should be “locfit” when the data frame has more than 10 thousands scores.\n\nsmo2 &lt;- smooth_lss(dat2, lss_var = \"lss_hostility\", date_var = \"date\", engine = \"locfit\")\n\nNow, let’s visualize hostility towards Ukraine expressed in Sputnik news over 2022.\n\nggplot(smo2, aes(x = date, y = fit)) + \n    geom_line() +\n    geom_ribbon(aes(ymin = fit - se.fit * 1.96, ymax = fit + se.fit * 1.96), alpha = 0.1) +\n    geom_vline(xintercept = as.Date(\"2022-02-24\"), linetype = \"dashed\") +\n    geom_vline(xintercept = as.Date(\"2022-09-21\"), linetype = \"dashed\") +\n    scale_x_date(date_breaks = \"months\", date_labels = \"%b\") +\n    labs(title = \"Hostility towards Ukraine\", x = \"Date\", y = \"Hostility\") +\n    theme_bw() +\n    annotate(\n    \"text\",\n    x = as.Date(\"2022-02-24\"),\n    y = Inf,\n    label = \"Full-scale invasion\",\n    size = 4,\n    vjust = 1.1,\n    hjust = -0.05) +\n    annotate(\"text\",\n    x = as.Date(\"2022-09-21\"),\n    y = Inf,\n    label = \"Military mobilization\\nin Russia\",\n    size = 4,\n    vjust = 1.1,\n    hjust = -0.05\n  )\n\n\n\n\n\n\n\n\n\n\nExample 3: Placement on the war-related dimensions\nLSS works also for cases when we define the seed words for one pole of our dimension only. For instance, we have a set of words associated with a particular theme, but we do not have a set of words which would define the opposite pole for this theme. So, LSS will place documents on the scale, where we have only +1 pole, defined by our seed words.\nIn the context of the ongoing Russia-Ukraine war, we can hypothesize that Russian propaganda articles mentioning Ukaine can be placed on the following latent dimensions: (1) ‘International’ (2) ‘Ground war operations’ (3) ‘World War 2’ (the analogy between WW2 and Russian invasion of Ukraine is often used by Russian propaganda).\nSo, we first define three dimensions of our theoretical interests with seed words.\n\nkeywords &lt;- list(\n    international = c(\"nato*\", \"peace*\", \"west*\", \"usa*\", \"europe*\", \"zelensk*\", \"biden*\", \"un*\", \"americ*\", \"washington*\", \"international*\", \"kyiv*\", \"mfa*\", \"russophob*\", \"britain*\", \"european*\", \"franc*\", \"nuclear*\", \"uk\")\n,\n  ground_war = c('militar*', 'combat*', 'operat*', 'special*', 'defen*', 'arm*', 'troop*', 'service*', 'territor*', 'battalion*', 'fleet*', 'commander*', 'lieutenant*', 'airborne*', 'crew*', 'artillery*', 'paratrooper*', 'command*', 'sergeant*', 'major*', 'tank*', 'soldier*')\n,\n  wwii = c('fascis*', 'victor*', 'great*', 'histor*', 'hero*', 'nazi*', 'soviet*', 'patriot*', 'memor*', 'homeland*', 'feat*', 'leningrad*', 'stalingrad*', 'invader*', 'grandfather*', 'fallen*')\n)\n\nkeywords &lt;- dictionary(keywords)\n\nNow, we run estimations of words’ polarities for each dimension.\n\nlss_international &lt;- textmodel_lss(dfmt, keywords[\"international\"])\n\nlss_ground_war &lt;- textmodel_lss(dfmt, keywords[\"ground_war\"])\n\nlss_wwii &lt;- textmodel_lss(dfmt, keywords[\"wwii\"])\n\n\nas.data.frame(head(coef(lss_ground_war), 10))\n\n           head(coef(lss_ground_war), 10)\nbrigade                         0.1601302\ncombat                          0.1391691\nairborne                        0.1335668\ninfantry                        0.1332081\nsoldiers                        0.1328685\ncolonel                         0.1310139\nattached                        0.1286344\nlieutenant                      0.1280001\ntrained                         0.1251352\ntraining                        0.1239563\n\n\n\ntextplot_terms(lss_international)\n\n\n\n\n\n\n\n\n\ntextplot_terms(lss_ground_war)\n\nWarning: ggrepel: 6 unlabeled data points (too many overlaps). Consider\nincreasing max.overlaps\n\n\n\n\n\n\n\n\n\n\ntextplot_terms(lss_wwii)\n\nWarning: ggrepel: 2 unlabeled data points (too many overlaps). Consider\nincreasing max.overlaps\n\n\n\n\n\n\n\n\n\n\n#dfmt_doc &lt;- dfm_group(dfmt)\n#dat &lt;- docvars(dfmt_doc)\n\ndat$lss_international &lt;- predict(lss_international, newdata = dfmt_doc)\ndat$lss_ground_war &lt;- predict(lss_ground_war, newdata = dfmt_doc)\ndat$lss_wwii &lt;- predict(lss_wwii, newdata = dfmt_doc)\n\n\nsmo3 &lt;- smooth_lss(dat, lss_var = \"lss_international\", date_var = \"date\", engine = \"locfit\")\n\n\nggplot(smo3, aes(x = date, y = fit)) + \n    geom_line() +\n    geom_ribbon(aes(ymin = fit - se.fit * 1.96, ymax = fit + se.fit * 1.96), alpha = 0.1) +\n    geom_vline(xintercept = as.Date(\"2022-02-24\"), linetype = \"dashed\") +\n    geom_vline(xintercept = as.Date(\"2022-09-21\"), linetype = \"dashed\") +\n    scale_x_date(date_breaks = \"months\", date_labels = \"%b\") +\n    labs(title = \"Engagement in Internaitonal rhetoric\", x = \"Date\", y = \"LSS International\") +\n    theme_bw() +\n    annotate(\n    \"text\",\n    x = as.Date(\"2022-02-24\"),\n    y = Inf,\n    label = \"Full-scale invasion\",\n    size = 4,\n    vjust = 1.1,\n    hjust = -0.05) +\n    annotate(\"text\",\n    x = as.Date(\"2022-09-21\"),\n    y = Inf,\n    label = \"Military mobilization\\nin Russia\",\n    size = 4,\n    vjust = 1.1,\n    hjust = -0.05\n  )\n\n\n\n\n\n\n\n\n\nsmo4 &lt;- smooth_lss(dat, lss_var = \"lss_ground_war\", date_var = \"date\", engine = \"locfit\")\n\nggplot(smo4, aes(x = date, y = fit)) + \n    geom_line() +\n    geom_ribbon(aes(ymin = fit - se.fit * 1.96, ymax = fit + se.fit * 1.96), alpha = 0.1) +\n    geom_vline(xintercept = as.Date(\"2022-02-24\"), linetype = \"dashed\") +\n    geom_vline(xintercept = as.Date(\"2022-09-21\"), linetype = \"dashed\") +\n    scale_x_date(date_breaks = \"months\", date_labels = \"%b\") +\n    labs(title = \"Engagement in Ground war rhetoric\", x = \"Date\", y = \"LSS Ground war\") +\n    theme_bw() +\n    annotate(\n    \"text\",\n    x = as.Date(\"2022-02-24\"),\n    y = Inf,\n    label = \"Full-scale invasion\",\n    size = 4,\n    vjust = 1.1,\n    hjust = -0.05) +\n    annotate(\"text\",\n    x = as.Date(\"2022-09-21\"),\n    y = Inf,\n    label = \"Military mobilization\\nin Russia\",\n    size = 4,\n    vjust = 1.1,\n    hjust = -0.05\n  )\n\n\n\n\n\n\n\n\n\nsmo5 &lt;- smooth_lss(dat, lss_var = \"lss_wwii\", date_var = \"date\", engine = \"locfit\")\n\nggplot(smo5, aes(x = date, y = fit)) + \n    geom_line() +\n    geom_ribbon(aes(ymin = fit - se.fit * 1.96, ymax = fit + se.fit * 1.96), alpha = 0.1) +\n    geom_vline(xintercept = as.Date(\"2022-02-24\"), linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-05-09\"), linetype = \"dashed\") +\n    geom_vline(xintercept = as.Date(\"2022-09-21\"), linetype = \"dashed\") +\n    scale_x_date(date_breaks = \"months\", date_labels = \"%b\") +\n    labs(title = \"Engagement in WW2 rhetoric\", x = \"Date\", y = \"LSS WW2\") +\n    theme_bw() +\n    annotate(\n    \"text\",\n    x = as.Date(\"2022-02-24\"),\n    y = Inf,\n    label = \"Full-scale invasion\",\n    size = 4,\n    vjust = 1.1,\n    hjust = -0.05) +\n    annotate(\"text\",\n    x = as.Date(\"2022-09-21\"),\n    y = Inf,\n    label = \"Military mobilization\\nin Russia\",\n    size = 4,\n    vjust = 1.1,\n    hjust = -0.05) + \n    annotate(\"text\",\n    x = as.Date(\"2022-05-09\"),\n    y = Inf,\n    label = \"WW2 Victory Day\",\n    size = 4,\n    vjust = 2.3,\n    hjust = -0.05\n  )\n\n\n\n\n\n\n\n\nWe can also plot trends for all dimensions on one graph.\n\n# --- Make a helper function for smoothing and labeling ---\nmake_smo &lt;- function(data, lss_var, facet_label) {\n  out &lt;- smooth_lss(data, lss_var = lss_var, date_var = \"date\", engine = \"locfit\")\n  out$facet &lt;- facet_label\n  return(out)\n}\n\n# --- Generate all smoothed datasets ---\nsmo_all &lt;- bind_rows(\n  make_smo(dat, \"lss_international\", \"International\"),\n  make_smo(dat, \"lss_ground_war\", \"Ground War\"),\n  make_smo(dat, \"lss_wwii\", \"WW2\"),\n)\n\nsmo_all$facet &lt;- factor(\n  smo_all$facet,\n  levels = c(\"Ground War\", \"International\", \"WW2\")\n)\n\n\nggplot(smo_all, aes(x = date, y = fit)) + \n  geom_line() +\n  geom_ribbon(\n    aes(ymin = fit - se.fit * 1.96, ymax = fit + se.fit * 1.96),\n    alpha = 0.1, colour = NA\n  ) +\n  geom_vline(xintercept = as.Date(\"2022-02-24\"), linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-09-21\"), linetype = \"dashed\") +\n  scale_x_date(date_breaks = \"months\", date_labels = \"%b\") +\n  labs(x = \"Date\", y = \"LSS Score\") +\n  theme_bw() +\n  theme(\n    axis.title.x = element_blank(),\n    strip.text = element_text(size = 14),\n    axis.text = element_text(size = 12),\n    text = element_text(size = 14)\n  ) +\n  facet_wrap(~ facet, scales = \"free_y\", ncol = 1) +\n\n  annotate(\n    \"text\",\n    x = as.Date(\"2022-02-24\"),\n    y = Inf,\n    label = \"Full-scale invasion\",\n    size = 3,\n    vjust = 1.1,\n    hjust = 0.08\n  ) +\n  annotate(\n    \"text\",\n    x = as.Date(\"2022-09-21\"),\n    y = Inf,\n    label = \"Military mobilization\",\n    size = 3,\n    vjust = 1.1,\n    hjust = 0.08\n  )\n\n\n\n\n\n\n\n\n\n\nKeyness analysis\n\n#texts(corp)[1:3]\n#docnames(corp)[1:3]\n#docnames(dfmt_doc)[1:3]\ndat %&gt;% arrange(desc(lss_ground_war)) %&gt;% select(docname) %&gt;% slice(1)\n\n      docname\n1 s1097818892\n\n\n\ntop_ground_war &lt;- texts(corp)[docnames(corp) == \"s1097818892\"]\n\nWarning: 'texts.corpus' is deprecated.\nUse 'as.character' instead.\nSee help(\"Deprecated\")\n\n\n\ndat %&gt;% arrange(lss_ground_war) %&gt;% select(docname) %&gt;% slice(1)\n\n      docname\n1 s1093775236\n\n\n\ntail_ground_war &lt;- texts(corp)[docnames(corp) == \"s1104348562\"]\n\nWarning: 'texts.corpus' is deprecated.\nUse 'as.character' instead.\nSee help(\"Deprecated\")\n\n\n\ndata_keyness &lt;- data.frame(\n  text = c(top_ground_war, tail_ground_war),\n  ground_war = c('top', 'tail'),\n  stringsAsFactors = FALSE\n)\n\n\ncorpus_keyness &lt;- corpus(data_keyness)\n\ntok_keyness &lt;- quanteda::tokens(corpus_keyness, what = \"word\",\n              remove_punct = TRUE,\n              remove_symbols = TRUE,\n              remove_numbers = TRUE,\n              remove_url = TRUE,\n              verbose = TRUE,\n              split_hyphens = TRUE,\n              split_tags = TRUE)\n\nCreating a tokens object from a corpus input...\n\n\n ...starting tokenization\n\n\n ...tokenizing 1 of 1 blocks\n\n\n ...segmenting into words\n\n\n ...334 unique types\n\n\n ...removing separators, punctuation, symbols, numbers, URLs \n\n\n ...complete, elapsed time: 0.029 seconds.\n\n\nFinished constructing tokens from 2 documents.\n\n#toks2_keyness &lt;- tokens_remove(tok_keyness, stopwords, padding = TRUE) \n\ndfm_keyness &lt;- dfm(tok_keyness, \n           tolower = TRUE,\n           verbose = TRUE)\n\nCreating a dfm from a tokens input...\n\n\n ...lowercasing\n\n\n ...found 2 documents, 306 features\n\n\n ...complete, elapsed time: 0.019 seconds.\n\n\nFinished constructing a 2 x 306 sparse dfm.\n\ndfm_keyness &lt;- dfm_remove(dfm_keyness, stopwords(\"en\"))\n\ndfm_keyness &lt;- dfm_remove(dfm_keyness, '')\n\ndfm_keyness &lt;- quanteda::dfm_wordstem(dfm_keyness)\n\ndfm_keyness &lt;- quanteda::dfm_group(dfm_keyness, groups = ground_war) \n\n\nresult_keyness &lt;- textstat_keyness(dfm_keyness, target = \"top\")\n\n\ntextplot_keyness(result_keyness, margin = 0.4, color = c(\"black\", \"grey\"), n = 10) +\n  theme(legend.position=\"bottom\", legend.text=element_text(size=10), plot.title = element_text(hjust = 0.5)) + ggtitle(\"Top vs Tail on Ground war\")"
  },
  {
    "objectID": "tutorial/lss_tutorial.html#corpus-2-russian-governors-addresses",
    "href": "tutorial/lss_tutorial.html#corpus-2-russian-governors-addresses",
    "title": "Introduction to Latent Semantic Scaling",
    "section": "Corpus 2: Russian governors’ addresses",
    "text": "Corpus 2: Russian governors’ addresses\n\nPreparing the text corpus\nNow, we will work with the text corpus of regional legislative addresses by Russian governors in 2007-2021, in Russian, from Baturo, Khokhlov, and Tolstrup (2024). The corpus can be downloaded here (save it in the same working directory as this .qmd file).\n\ngov_corpus &lt;- readRDS(\"data/gov_corpus.RDS\")\nndoc(gov_corpus)\n\n[1] 924\n\n\n\n\nExample 4: Governors’ engagement in Putin’s agenda in Russia\nDowlonad dictionary from here (save it as dictionary_putin.yml in the same working directory as this .qmd file).\n\ndict_putin &lt;- dictionary(file = \"data/dictionary_putin.yml\")\n\nWarning in readLines(con, warn = readLines.warn): incomplete final line found\non 'data/dictionary_putin.yml'\n\n\n\ndict_putin[\"putin_agenda2\"]\n\nDictionary object with 1 key entry.\n- [putin_agenda2]:\n  - путин, путина, путиным, крым, украин, патриот, побед, фаши, суверен, традицион\n\n\nPrepare dfm for the analysis.\n\ntoks2 &lt;- gov_corpus %&gt;% \n    corpus_reshape(\"sentences\") %&gt;% \n    tokens(remove_punct = TRUE) %&gt;% \n    tokens_remove(pattern = stopwords(\"ru\", source = \"snowball\")) \ndfmt2 &lt;- toks2 %&gt;% \n    dfm() %&gt;%\n    dfm_select(\"^\\\\p{L}+$\", valuetype = \"regex\", min_nchar = 2) %&gt;% \n    dfm_trim(min_termfreq = 5)\n\ndfmt2 &lt;- dfm_remove(dfmt2, \"\")\n    \ndfmt2 &lt;- dfm_wordstem(dfmt2, language = \"russian\")\n\nCalculate polarity scores for words on the “putin_agenda” dimension.\n\nputin_agenda &lt;- textmodel_lss(dfmt2, dict_putin[\"putin_agenda2\"], cache = FALSE)\n\nSave top terms associated with seed words defining Putin’s agenda.\n\ncloud &lt;- head(coef(putin_agenda), 100)\ncloud&lt;-as.data.frame(cloud)\ncloud$word &lt;- rownames(cloud)\n\nVisualize the word cloud for top terms associated with Putins’ agenda.\n\nlibrary(wordcloud)\n\nLoading required package: RColorBrewer\n\nwordcloud(cloud$word,cloud$cloud,scale=c(2,0.1),random.order=FALSE, rot.per=0.35, use.r.layout=FALSE, max.words=60)\n\n\n\n\n\n\n\n\n\ntextplot_terms(putin_agenda, highlighted = c('эконом*', 'америк*', 'социал*', 'полит*', dict_putin[\"putin_agenda2\"]))\n\n\n\n\n\n\n\n\nNow, let’s calculate polarity scores for documents.\n\ndat  &lt;- docvars(dfmt)\ndat$putin_agenda &lt;- predict(putin_agenda, newdata = dfmt)\n\n\n\nExample 5: LSS Scores in regression models\nLet’s download the dataset with lss scores and speaker-level and region-level covariates used in Baturo, Khokhlov, and Tolstrup (2024).\n\ndf &lt;- readRDS(\"data/data_governors.rds\")\n\nWe use LSS scores as a dependent variable (lss_putinagenda2) in a regression model with governor- and region-level covariates.\n\nm1 &lt;- plm(lss_putinagenda2 ~  lognextdays + preselec_window + transfers_log + aftercrimea + reg_deputies_ur_share + reg_grp_pc_log + gov_background_main_business + appointed_first_by_putin_bin + siloviki , data = df[ which(!complete.cases(df$noposlanie) & !complete.cases(df$written)), ], effect = \"individual\", model = \"within\", index='region')\n\n\nmodelsummary(\n  m1,\n  statistic = \"({std.error})\",\n  stars = TRUE,\n  output = \"gt\",\n  gof_omit = \"IC|Log|Adj\",\n  coef_map = c(\n    lognextdays = \"Log days to election\",\n    preselec_window = \"Pre-election window\",\n    transfers_log = \"Log federal transfers\",\n    aftercrimea = \"Post-Crimea\",\n    reg_deputies_ur_share = \"UR deputies share\",\n    reg_grp_pc_log = \"Log GRP per capita\",\n    gov_background_main_business = \"Governor business background\",\n    appointed_first_by_putin_bin = \"First appointed by Putin\",\n    siloviki = \"Siloviki background\"\n  )\n)\n\n\n\n\n\n\n\n\n(1)\n\n\n\n\nLog days to election\n0.000\n\n\n\n(0.007)\n\n\nPre-election window\n-0.009\n\n\n\n(0.018)\n\n\nLog federal transfers\n0.037*\n\n\n\n(0.014)\n\n\nPost-Crimea\n0.033+\n\n\n\n(0.019)\n\n\nUR deputies share\n0.000\n\n\n\n(0.001)\n\n\nLog GRP per capita\n-0.003\n\n\n\n(0.005)\n\n\nGovernor business background\n-0.073**\n\n\n\n(0.024)\n\n\nFirst appointed by Putin\n0.039+\n\n\n\n(0.020)\n\n\nSiloviki background\n-0.095**\n\n\n\n(0.034)\n\n\nNum.Obs.\n680\n\n\nR2\n0.052\n\n\nRMSE\n0.14\n\n\n\n+ p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001"
  },
  {
    "objectID": "tutorial/lss_tutorial.html#footnotes",
    "href": "tutorial/lss_tutorial.html#footnotes",
    "title": "Introduction to Latent Semantic Scaling",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nengine should be “locfit” when the data frame has more than 10 thousands scores.↩︎"
  },
  {
    "objectID": "slides/lss_slides.html#what-is-latent-semantic-scaling",
    "href": "slides/lss_slides.html#what-is-latent-semantic-scaling",
    "title": "Introduction to Latent Semantic Scaling",
    "section": "What is Latent Semantic Scaling?",
    "text": "What is Latent Semantic Scaling?\nA semi-supervised scaling technique (Watanabe 2021).\n\nThe user provides seed words to define the scale.\nThe algorithm estimates polarity of words in the corpus based on their semantic proximity to seed words.\nDocuments are then located on a unidimensional scale."
  },
  {
    "objectID": "slides/lss_slides.html#why-lss",
    "href": "slides/lss_slides.html#why-lss",
    "title": "Introduction to Latent Semantic Scaling",
    "section": "Why LSS?",
    "text": "Why LSS?\n\nRequires only a small set of seed words regardless of the size of the corpus\nLearns the semantic relationship between words without manual supervision\nWorks well for non-European languages\nFitted with the entire corpus –&gt; independent of temporality of language usage –&gt; great for longitudinal analysis"
  },
  {
    "objectID": "slides/lss_slides.html#seed-words",
    "href": "slides/lss_slides.html#seed-words",
    "title": "Introduction to Latent Semantic Scaling",
    "section": "Seed Words",
    "text": "Seed Words\n\nSeed words are a small set of words that users choose to define the quantity that LSS measures.\nGood seed words should have:\n\nstrong polarity\nsmall ambiguity\ncorpus independence"
  },
  {
    "objectID": "slides/lss_slides.html#seed-words-selection",
    "href": "slides/lss_slides.html#seed-words-selection",
    "title": "Introduction to Latent Semantic Scaling",
    "section": "Seed Words Selection",
    "text": "Seed Words Selection\n\nMake a list of candidate polarity words related to the target dimension.\nSelect candidates that have the strongest polarity and the smallest ambiguity.\nFit a LSS model with only one candidate and test if polarity scores of words are intuitively correct.\nCombine all the candidates that pass the checks to form a seed word set\nCheck the validity of seed words by comparing scores produced by manual and machine coding."
  },
  {
    "objectID": "slides/lss_slides.html#polarity-scores-of-words-in-corpus",
    "href": "slides/lss_slides.html#polarity-scores-of-words-in-corpus",
    "title": "Introduction to Latent Semantic Scaling",
    "section": "Polarity Scores of Words in Corpus",
    "text": "Polarity Scores of Words in Corpus\n\nLSS employs the word-embedding technique to estimate semantic proximity between seed words and other words in the corpus.\nWord-embeddings produce low-dimensional representations of word semantics, “word vectors”, based on cooccurrences of words within sentences or word-windows.\nAfter obtaining word vectors, LSS computes polarity scores of words based on their proximity to seed words, weighted by their user-provided polarity."
  },
  {
    "objectID": "slides/lss_slides.html#word-weighting-illustration",
    "href": "slides/lss_slides.html#word-weighting-illustration",
    "title": "Introduction to Latent Semantic Scaling",
    "section": "Word Weighting Illustration",
    "text": "Word Weighting Illustration\n\nConceptual illustration of word weighting by seed words. The arrow is the sentiment dimension in the semantic space and circles are proximities of positive seed (“good”) words and negative seed words (“bad”) to “crisis”, which is projected on the sentiment dimension (Watanabe 2021, 86)."
  },
  {
    "objectID": "slides/lss_slides.html#polarity-scores-illustration",
    "href": "slides/lss_slides.html#polarity-scores-illustration",
    "title": "Introduction to Latent Semantic Scaling",
    "section": "Polarity Scores Illustration",
    "text": "Polarity Scores Illustration\n\nDistribution of polarity scores and word frequencies on the “hostility” dimension (Trubowitz and Watanabe 2021, 858)."
  },
  {
    "objectID": "slides/lss_slides.html#polarity-scores-of-documents",
    "href": "slides/lss_slides.html#polarity-scores-of-documents",
    "title": "Introduction to Latent Semantic Scaling",
    "section": "Polarity Scores of Documents",
    "text": "Polarity Scores of Documents\n\nLSS predicts polarity scores of documents by weighting word polarity scores by their frequency in the documents.\nDocuments’ polarity scores are continuously and symmetrically distributed around the mean, so they are recentered by the global mean, μ = 0, and rescaled by standard deviation, σ = 1, to make it easier for users to interpret.\n\\(\\rightarrow\\) great for post-hoc regression analysis to incorporate covariates."
  },
  {
    "objectID": "slides/lss_slides.html#application-examples-1",
    "href": "slides/lss_slides.html#application-examples-1",
    "title": "Introduction to Latent Semantic Scaling",
    "section": "Application Examples 1",
    "text": "Application Examples 1\n\n\n\n\n\n\n\n\n\nArticle\nDimensions & seed words\nData\n\n\n\n\nTrubowitz and Watanabe (2021)\n“Hostility” (+1: “adversary,” “enemy,” “foe,” “hostile; −1: “aid,” “ally,” “friend,” “peaceful”)\nNew York Times news summaries (N = 387,896)\n\n\nRauh (2021)\n“Emergency” vs “Normality” (+1: “emergency”, “crisis”, “danger”, “peril”, “hazard”, “threat”, “risk”, “disaster”, “uncertainty”, “uncertain”; -1: “normality”, “normal”, “safety”, “stability”, “regularity”, “routine”, “calm”, “usual”, “certainty”, “certain”)\nSpeeches of EU leaders and heads of states (N = 19,541)"
  },
  {
    "objectID": "slides/lss_slides.html#application-examples-2",
    "href": "slides/lss_slides.html#application-examples-2",
    "title": "Introduction to Latent Semantic Scaling",
    "section": "Application Examples 2",
    "text": "Application Examples 2\n\n\n\n\n\n\n\n\n\nArticle\nDimensions & seed words\nData\n\n\n\n\nBaturo and Gray (2024)\n“Global agenda” rhetoric (“cooperat,” “global,” “agenda,” “united nations,” “organization”, “assembly”)\nSpeeches from the United Nations General Assembly (1946-2019), grouped by speakers (N = 9,959)\n\n\nBaturo, Khokhlov, and Tolstrup (2024)\n“Putin’s agenda” (“putin,” “sovereign,” “tradition-,” “crimea,” “ukraine,” “patriot-,” “victory,” “fascis-”)\nAnnual regional legislative addresses by governors in Russia, 2007-2021 (N = 924)"
  },
  {
    "objectID": "slides/lss_slides.html#analytical-pipeline-example",
    "href": "slides/lss_slides.html#analytical-pipeline-example",
    "title": "Introduction to Latent Semantic Scaling",
    "section": "Analytical Pipeline Example",
    "text": "Analytical Pipeline Example\n\nPhases of semi-supervised classification of text (Trubowitz and Watanabe 2021)."
  },
  {
    "objectID": "slides/lss_slides.html#example-polarity-scores",
    "href": "slides/lss_slides.html#example-polarity-scores",
    "title": "Introduction to Latent Semantic Scaling",
    "section": "Example: Polarity Scores",
    "text": "Example: Polarity Scores\n\n\n\n\n\n\n\n\nSeed words\n\n\n\n\n“cooperat,” “global,” “agenda,” “united nations,” “organization”, “assembly”\n\n\n\n\n\n\n\n\nGlobal agenda terms (Baturo and Gray 2024, 732). Figure includes 100 top terms with the highest estimated coefficients, that is, most lexically similar to the “global agenda” seed words."
  },
  {
    "objectID": "slides/lss_slides.html#example-keyness-as-a-validation",
    "href": "slides/lss_slides.html#example-keyness-as-a-validation",
    "title": "Introduction to Latent Semantic Scaling",
    "section": "Example: Keyness as a Validation",
    "text": "Example: Keyness as a Validation\n\nLexical difference (Baturo and Gray 2024, 735). Note: The keyness analyses isolate words that frequently appear in a given text(s), capturing the degree to which a given word is “key” overall, in contrast to another text(s)."
  },
  {
    "objectID": "slides/lss_slides.html#lss-weaknesses",
    "href": "slides/lss_slides.html#lss-weaknesses",
    "title": "Introduction to Latent Semantic Scaling",
    "section": "LSS Weaknesses",
    "text": "LSS Weaknesses\n\nNot as robust to external corpora as the dictionary approach because it estimates semantic proximity between words on the given corpus only.\nPredicts individual polarity scores with relatively large errors, esp. in short documents, because it is a linear and naive model.\nBut grouped LSS scores (e.g., by speaker) are strongly correlated with manual scores in tests \\(\\rightarrow\\) measurement errors are negligible in aggregated-level analysis"
  },
  {
    "objectID": "slides/lss_slides.html#references",
    "href": "slides/lss_slides.html#references",
    "title": "Introduction to Latent Semantic Scaling",
    "section": "References",
    "text": "References\n\n\n\n\nBaturo, Alexander, and Julia Gray. 2024. “Leaders in the United Nations General Assembly: Revitalization or Politicization?” The Review of International Organizations 19 (4): 721–52. https://doi.org/10.1007/s11558-023-09524-1.\n\n\nBaturo, Alexander, Nikita Khokhlov, and Jakob Tolstrup. 2024. “Playing the Sycophant Card: The Logic and Consequences of Professing Loyalty to the Autocrat.” American Journal of Political Science 69 (3): 1180–95. https://doi.org/10.1111/ajps.12909.\n\n\nRauh, Christian. 2021. “Supranational Emergency Politics? What Executives’ Public Crisis Communication May Tell Us.” Journal of European Public Policy 29 (6): 966–78. https://doi.org/10.1080/13501763.2021.1916058.\n\n\nTrubowitz, Peter, and Kohei Watanabe. 2021. “The Geopolitical Threat Index: A Text-Based Computational Approach to Identifying Foreign Threats.” International Studies Quarterly 65 (3): 852–65. https://doi.org/10.1093/isq/sqab029.\n\n\nWatanabe, Kohei. 2021. “Latent Semantic Scaling: A Semisupervised Text Analysis Technique for New Domains and Languages.” Communication Methods and Measures 15 (2): 81–102. https://doi.org/10.1080/19312458.2020.1832976."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Latent Semantic Scaling",
    "section": "",
    "text": "Materials for teaching and replication.\n\n📘 Tutorial: https://nikitakhokhlov.github.io/lss_workshop_ucd/lss_tutorial.html\n🛠️ Materials for tutorial (to run the code locally): https://github.com/nikitakhokhlov/lss_workshop_ucd/tree/main/tutorial\n📊 Slides: https://nikitakhokhlov.github.io/lss_workshop_ucd//lss_slides.html"
  }
]